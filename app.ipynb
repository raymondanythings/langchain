{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1,streaming=True,callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm,return_messages=True, max_token_limit=120)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([('system','You are a helpful AI talking to a human') ,MessagesPlaceholder(variable_name=\"history\"), (\"human\", \"{question}\")])\n",
    "\n",
    "def load_memory(input):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\":question})\n",
    "    memory.save_context({\"input\": question} , {\"output\" : result.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Raymond! How can I assist you today?content='Nice to meet you, Raymond! How can I assist you today?'\n",
      "That's great to know, Raymond! Seoul is a vibrant and bustling city with a rich history and culture. Is there anything specific you'd like to know or talk about regarding Seoul or anything else?content=\"That's great to know, Raymond! Seoul is a vibrant and bustling city with a rich history and culture. Is there anything specific you'd like to know or talk about regarding Seoul or anything else?\"\n"
     ]
    }
   ],
   "source": [
    "invoke_chain('My name is raymond')\n",
    "invoke_chain(\"I living in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Raymond.content='Your name is Raymond.'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")\n",
    "# memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"history\": \"Human: Hi I'm Nicolas, I live in South Korea\\nAI: Wow that is so cool!\\nHuman: Nicolas likes Kimchi\\nAI: I wish I could go!!\",\n",
      "  \"input\": \"what does nicolas like?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an AI assistant reading the transcript of a conversation between an AI and a human. Extract all of the proper nouns from the last line of conversation. As a guideline, a proper noun is generally capitalized. You should definitely extract all names and places.\\n\\nThe conversation history is provided just in case of a coreference (e.g. \\\"What do you know about him\\\" where \\\"him\\\" is defined in a previous line) -- ignore items mentioned there that are not in the last line.\\n\\nReturn the output as a single comma-separated list, or NONE if there is nothing of note to return (e.g. the user is just issuing a greeting or having a simple conversation).\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: how's it going today?\\nAI: \\\"It's going great! How about you?\\\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \\\"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\\\"\\nLast line:\\nPerson #1: i'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\\nOutput: Langchain\\nEND OF EXAMPLE\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: how's it going today?\\nAI: \\\"It's going great! How about you?\\\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \\\"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\\\"\\nLast line:\\nPerson #1: i'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I'm working with Person #2.\\nOutput: Langchain, Person #2\\nEND OF EXAMPLE\\n\\nConversation history (for reference only):\\nHuman: Hi I'm Nicolas, I live in South Korea\\nAI: Wow that is so cool!\\nHuman: Nicolas likes Kimchi\\nAI: I wish I could go!!\\nLast line of conversation (for extraction):\\nHuman: what does nicolas like?\\n\\nOutput:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] [742ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Nicolas, Kimchi\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Nicolas, Kimchi\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 440,\n",
      "      \"completion_tokens\": 5,\n",
      "      \"total_tokens\": 445\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": \"fp_b28b39ffa8\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [743ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Nicolas, Kimchi\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Nicolas: Nicolas lives in South Korea. Nicolas likes Kimchi.')]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"what does nicolas like?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"How far is Korea from Aegentina?\",\"I don't know! Super far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Nicolas, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!'),\n",
       "  HumanMessage(content='South Korea is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!'),\n",
       "  HumanMessage(content='How far is Korea from Aegentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\")]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"summary\": \"\",\n",
      "  \"new_lines\": \"Human: Hi I'm Nicolas, I live in South Korea\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n\\n\\nNew lines of conversation:\\nHuman: Hi I'm Nicolas, I live in South Korea\\n\\nNew summary:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] [1ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The human introduces himself as Nicolas and mentions that he lives in South Korea.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The human introduces himself as Nicolas and mentions that he lives in South Korea.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The human introduces himself as Nicolas and mentions that he lives in South Korea.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "add_message(\"How far is Brazil from Aegentina?\",\"I don't know! Super far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human introduces himself as Nicolas and mentions that he lives in South Korea.'),\n",
       "  AIMessage(content='Wow that is so cool!'),\n",
       "  HumanMessage(content='South Korea is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!'),\n",
       "  HumanMessage(content='How far is Korea from Aegentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\"),\n",
       "  HumanMessage(content='How far is Brazil from Aegentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\"),\n",
       "  HumanMessage(content='How far is Brazil from Aegentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\"),\n",
       "  HumanMessage(content='How far is Brazil from Aegentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\")]}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
